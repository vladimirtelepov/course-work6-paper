\section{Сбор набора данных} \label{5-dataset}

Поиск по открытым источникам размеченного набора данных, содержащего бинарный код функций, не дал результатов, поэтому было принято решение подготовить набор данных самостоятельно, используя открытый исходный код проектов на платформе Github. Требовалось выбрать компилируемый язык, на котором в дальнейшем предполагалось собирать проекты. При этом для автоматизации процесса cбора данных необходимо, чтобы процесс сборки проектов был унифицированным. Исходя из описанных требований, был выбран язык Rust, в котором де-факто стандартным инструментом для сборки проектов является Cargo. Чтобы определить набор классов, которым могут принадлежать функции, было решено применить алгоритм кластеризации и посмотреть типичных представителей для каждого кластера. После этого появляется возможность автоматически построить соответствие между скомпилированными функциями и полученными классами, т.е автоматически разметить набор данных. Для построения алгоритма кластеризации использовались сигнатуры функций, методов и комментарии к ним из проектов на языках C/C++/Rust. Код программной реализации находится в открытом доступе\footnote{\url{https://github.com/vladimirtelepov/course-work5}}.

\subsection{Сбор исходных кодов проектов с Github}
Github предоставляет для взаимодействия REST API. Для удобства была использована библиотека PyGithub. Искались репозитории с кодом на языках C/C++/Rust, выдача сортировалась по количеству оценок, брались репозитории с наибольшим количеством оценок. Для кластеризации сохранялись только файлы с расширениями .c, .cpp, .cc, .h, .rs. В итоге был собран код более чем из 16000 репозиториев.

\subsection{Подготовка данных для кластеризации}
Для того чтобы найти сигнатуры функций и комментарии к ним в полученных на предыдущем шаге файлах, использовались библиотеки clang и syn. Для уменьшения времени работы код был распараллелен с помощью библиотек multiprocessing, threading. В результате было собрано более 3.5 миллионов уникальных сигнатур и комментариев.

\subsection{Кластеризация}
Сначала имена функций, семантически состоящие из нескольких частей, разбивались на несколько слов перебором по различным стилям написания (camelCase, underscores, PascalCase). Все слова приводились к нижнему регистру, и удалялись все символы-цифры. Тем не менее по завершении описанных операций оставались слова, семантически состоящие из нескольких частей. Поэтому была произведена сегментация всех полученных ранее слов с помощью алгоритма из книги Beatiful Data\cite{SegaranHammerbacher2009}. Предлагается следующая модель:

Последовательность слов $w_1..w_n$ рассматривается как мешок слов, т.е слова в последовательности считаются независимыми, поэтому $P(w_1...w_n) = P(w_1) * P(w_2) * ... * P(w_n)$. Разбиение слова $w$ на слова $w_1..w_n$ производится с помощью максимизации вероятности наблюдения последовательности слов $w_1..w_s, s <= m,$ где $m$ -- длина слова $w, w = \overline{w_1..w_s}$. Оценка вероятностей встречаемости слов обычно берется из готовых словарей, однако, из-за специфики именования функций, вероятности были оценены подсчетом частоты встречаемости слов в собранном наборе данных. Для того чтобы различать слова, в написании которых есть ошибки, от слов, не встретившихся в корпусе, и присваивать им разные вероятности, применяется аналог сглаживания вероятности Гуда-Тьюринга. Если слово не встретилось в корпусе текста, то будем давать оценку вероятности на основе вероятности встретить слово такой же длины:
Пусть $l$ -- длина слова $w$, не встретившегося в словаре, $P_{ones}(l)$ -- вероятность случайно выбрать слово длины $l$ среди слов, встретившихся в наборе данных один раз, $c_{prior}$ -- константа, позволяющая сделать незнакомые слова менее вероятными, $l_{max}$ -- максимальная длина слова, что $P_{ones}(l_{max}) > 0$. Тогда

$P_{unseen}(w) = \begin{cases}
c_{prior} * P_{ones}(l) & \quad P_{ones}(l) > 0 , \\
c_{prior} * \frac{1}{26}^l & \quad P_{ones}(l) = 0, l < l_{max}, \\
c_{prior} * P_{ones}(l_{max}) * \frac{1}{26}^{(l-l_{max})} & \quad P_{ones}(l) = 0, l >= l_{max}
\end{cases}$

Далее производилась лемматизация для приведения слов к нормальной форме и уменьшения размера словаря. Использовалась библиотека spacy и модель для английского текста. После этого удалялись слишком короткие, длинные, редкие и стоп-слова. Был получен словарь встречаемости, состоящий примерно из 50000 слов. Далее типы аргументов и возвращаемого значения были унифицированы. Затем к полученным векторам токенов применялась модель TF-IDF с логарифмическим преобразованием для TF компоненты. Использовалась библиотека sklearn. После этого применялся алгоритм кластеризации k-means из той же библиотеки, количество кластеров было задано 500. После этого брались "типичные представители" для каждого кластера как ближайшие к центроидам векторы. Алгоритм k-means -- фактически единственный метод, который отработал без превышения лимита по времени/используемым ресурсам на собранном наборе данных.

\subsection{Оценка качества кластеризации}
Для оценки качества кластеризации были просмотрены "значимые" слова в каждом кластере, то есть те, у которых наблюдался наибольший вес при усреднении векторных представлений по всем экземплярам кластера. Неоднократно имела место ситуация, когда 3 самых значимых слова в одном кластере частично совпадают с 3 самыми значимыми словами в другом кластере. Среди полученных кластеров имелись также очень маленькие кластеры и несколько очень больших, которые тяжело интерпретировать. Таким образом, приемлемое качество кластеризации не было достигнуто.

\subsection{Применение других методов обучения без учителя для группировки функций}
Были опробованы методы тематической классификации LSA, PLSA, LDA. Кроме того, были протестированы различные преобразования матрицы tf-idf перед подачей векторов алгоритму k-means: было произведено сокращение размерности пространства с помощью метода главных компонент, токенизированные представления сигнатур подавались предобученной модели BERT, брались значения векторов с последних 4 слоев и усреднялись, затем полученные последовательности усреднялись с помощью взвешивания по tf-idf соответствующей сигнатуры. 

Полученные кластеры также было сложно интерпретировать, поэтому было решено отказаться от методов обучения без учителя и выделить классы вручную.

\subsection{Выделение классов}
Для выделения классов было принято решение оставить только названия функций.
Выделение классов происходило следующим образом:
\begin{enumerate}
    \item просматривался список самых частых слов, из них первые 20-30 слов либо относились к уже выбранным классам или к стоп-словам, либо образовывали новые классы, либо пропускались;
    \item затем производилась разметка всех неразмеченных функций: для каждого класса считалось число слов, входящих в токенизированное название функции (слово может входить несколько раз), и выбирался класс с наибольшим количеством совпадений. При этом как минимум половина слов из названия функции должна была состоять в этом классе. Если таких классов было несколько, то выбирался случайный класс;
    \item далее процесс повторялся. 
\end{enumerate}

В итоге было отобрано 46 классов функций (подробнее про классы см. Приложение). Таким образом удалось разметить 1.5 миллиона функций.

\subsection{Выбор функций и разметка}
После сборки проектов, они компилировались с помощью cargo. Из исходного файла проекта выбирались имена функций. Каждый бинарный файл (ELF-файл, разделяемая или статическая библиотека, библиотека .rlib) подавался на вход программе mcsema-disass. Полученный граф потока управления подавался программе mcsema-lift. В конце биткод переводился в llvm-ir с помощью llvm-dis. В полученном файле искался код функций из исходных файлов с помощью регулярного выражения, содержащего выбранные имена, и сопоставления префикса названия функции и имени бинарного файла. По имени функции, используя описанный выше алгоритм, определялся класс, которому принадлежала функция. Для уменьшения времени работы код был распараллелен с помощью библиотек multiprocessing, threading.

